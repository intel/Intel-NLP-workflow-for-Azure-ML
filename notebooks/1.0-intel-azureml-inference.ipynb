{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2022 Intel Corporation\n",
    " \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    " \n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "Unless required by applicable law or agreed to in writing,\n",
    "software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions\n",
    "and limitations under the License.\n",
    " \n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Description\n",
    "\n",
    "Version: 1.0\n",
    "Date: Sep 20, 2022\n",
    "\n",
    "This notebook outlines the general usage of quantized (INT8) NLP Inference using Intel's CPU, PyTorch - with IPEX optimization, Intel Neural Compressor, and HuggingFace model on Azure Machine Learning platform. The trained BERT base model is further quantized by Intel Neural Compressor and converted into ONNX format.\n",
    "\n",
    "Users may wish to base on parts of the code and customize those to suit their purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "Log in Azure - please go to the terminal/console and use the command below to login Azure. Follow the instructions shown in the terminal to perform interactive authentication.\n",
    "\n",
    "Command:\n",
    "'az login'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create/Load the Azure Machine Learning workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config('./config.json')\n",
    "    print('Loaded existing workspace configuration')\n",
    "except:\n",
    "    ws = Workspace.create(name='intel_azureml_ws',\n",
    "            subscription_id='----USER AZURE SUBSCRIPTION ID----',  #Please fill in the azure-subscription-id \n",
    "            resource_group='intel_azureml_resource',    #\n",
    "            create_resource_group=True,\n",
    "            location='westus2'\n",
    "            )\n",
    "    ws.write_config(path=\"./\", file_name=\"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare the materials for quantizing trained HuggingFace model using Intel Neural Compressor\n",
    "In order to quantize the trained HuggingFace model, users have to prepare 2 files:\n",
    "1. INC config file\n",
    "2. Trained HuggingFace PyTorch model\n",
    "\n",
    "For the INC config file, we have prepared the ../src/inference_container/config/ptq.yaml file for users. It specified the operation (post quantization) to be performed by Intel Neural Compressor.\n",
    "\n",
    "The trained model should be downloaded automatically under './fp32_model_output' from the previous steps. If that is not the case, users may need to go to the webpage of Azure Machine and download the trained HuggingFace PyTorch model. Go to the webpage of Azure Machine Learning. 'work_space_name' -> 'Jobs' -> 'the_jobs_id' -> 'Outputs + logs' -> 'outputs' -> 'trained_model'\n",
    "\n",
    "The two directory and file will be uploaded through the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, ScriptRunConfig, Environment, Experiment\n",
    "from azureml.core.runconfig import MpiConfiguration, PyTorchConfiguration\n",
    "from azureml.core import Workspace\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.core import Dataset\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "Dataset.File.upload_directory(src_dir='../src/inference_container/config', \n",
    "                              target=DataPath(datastore, \"/inc/ptq_config\"),\n",
    "                              overwrite=True\n",
    "                             )\n",
    "Dataset.File.upload_directory(src_dir='./fp32_model_output/outputs/trained_model', \n",
    "                              target=DataPath(datastore, \"/trained_fp32_hf_model\"),\n",
    "                              overwrite=True\n",
    "                             )\n",
    "\n",
    "remote_inc_config =  Dataset.File.from_files(path=(datastore, '/inc/ptq_config'))\n",
    "remote_fp32_model = Dataset.File.from_files(path=(datastore, '/trained_fp32_hf_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2b: Remove the local /fp32_model_output folder\n",
    "It is necessary to remove the ./fp32_model_output folder to avoiding triggering error (exceeds 300 MB for the experiment snapshots).\n",
    "\n",
    "Details of the error can be referred as the following webpage:\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-save-write-experiment-files#limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('./fp32_model_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Start quantizting the model\n",
    "Setup the cluster and environment for launch a quantization job. To change the quantization or use more Intel Neural Compressor features (e.g.: distillation, pruning etc.), users may wish to modify the inc_quantization.py and the related configuration file (i.e. ptq.yaml).\n",
    "\n",
    "Note: For quantization, initiate one single node is sufficient for the quantization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "from azureml.core import Workspace, Environment\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core import Image\n",
    "\n",
    "#initiate a node for quantization\n",
    "cpu_cluster_name = \"cpuCluster1xD64DSV4\"\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D64DS_V4', max_nodes=1) #Ddsv4-series run on the 3rd Generation Intel速 Xeon速 Platinum 8370C (Ice Lake) or the Intel速 Xeon速 Platinum 8272CL (Cascade Lake).\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "cpu_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "#initiate the environment for quantization\n",
    "azure_ddp_ipex_hf_environment = Environment.get(ws, 'azure_ddp_ipex_hf_environment')\n",
    "azure_ddp_ipex_hf_environment.python.user_managed_dependencies=True\n",
    "\n",
    "#Setup the parameters for quantization\n",
    "script_params = [\n",
    "    '--inc_config_path',\n",
    "    remote_inc_config.as_named_input('inc_config').as_mount(),\n",
    "    '--inc_config_filename',\n",
    "    'ptq.yaml',\n",
    "    '--fp32_model_path',\n",
    "    remote_fp32_model.as_named_input('fp32_hf_model_path').as_mount(),\n",
    "    '--model_name',\n",
    "    'bert-base-uncased'\n",
    "]\n",
    "\n",
    "run_config = ScriptRunConfig(\n",
    "  source_directory= '../src/inference_container',\n",
    "  script='inc_quantization.py',\n",
    "  compute_target=cpu_cluster,\n",
    "  environment=azure_ddp_ipex_hf_environment,\n",
    "  arguments = script_params\n",
    ")\n",
    "\n",
    "# submit the run configuration to start the job\n",
    "run = Experiment(ws, \"INC_PTQ\").submit(run_config)\n",
    "run.wait_for_completion(show_output=True)\n",
    "run.download_files(output_directory='quantized_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Deploy the model \n",
    "There are multiple steps:\n",
    "1. Users can locate the quantized model downloaded in 'output' folder. If the directory does not exist, please use the webpage of the Azure Machine \n",
    "Learning Platoform to download the model to local directory - 'work_space_name' -> 'Jobs' -> 'the_jobs_id' -> 'Outputs + logs' -> 'outputs'.\n",
    "2. Register the quantized model to the Azure ML platform.\n",
    "3. Implement a score.py file to define the data preprocessing and post-processing at the end-point. It will also define the behavior of how the model infernece.\n",
    "\n",
    "Inside score_hf.py, specify the number of physical cores for the environment variable GOMP_CPU_AFFINITY. The best configuration found for Standard_D16_v5 is currently set as default, but users may choose to explore different numbers of physical cores for different machines. For more information on number of physical cores for different machines, please visit:https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-general\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, AksCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.webservice import AksWebservice, Webservice\n",
    "\n",
    "\n",
    "model_dir = 'quantized_model/outputs'\n",
    "model = Model.register(workspace = ws,\n",
    "                       model_path = model_dir,\n",
    "                       model_name = \"inc_ptq_bert_model_mrpc\",\n",
    "                       tags = {\"Model\": \"inc_ptq_bert_model_mrpc\"},\n",
    "                       description = \"Quantized HuggingFace Model\",)\n",
    "\n",
    "azure_ddp_ipex_hf_environment = Environment.get(ws, name='azure_ddp_ipex_hf_environment')\n",
    "azure_ddp_ipex_hf_environment.python.user_managed_dependencies=True\n",
    "azure_ddp_ipex_hf_environment.inferencing_stack_version = \"latest\"\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"../src/inference_container/score_hf.py\", environment=azure_ddp_ipex_hf_environment)\n",
    "\n",
    "#Create a AKS cluster\n",
    "try:\n",
    "    inference_node = AksCompute(workspace=ws, name=\"infericelake2\")\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    prov_config = AksCompute.provisioning_configuration(vm_size = \"Standard_D16_v5\", agent_count=3, location=\"westus\")\n",
    "    inference_node = ComputeTarget.create(workspace = ws, name = 'infericelake2', provisioning_configuration=prov_config)\n",
    "    inference_node.wait_for_completion(show_output=True)\n",
    "\n",
    "deployment_config = AksWebservice.deploy_configuration(cpu_cores=4, memory_gb=16) # Specify the resources for this deployment\n",
    "\n",
    "service_name = 'hf-aks-1'\n",
    "print(\"Service\", service_name)\n",
    "service = Model.deploy(ws, service_name, [model], inference_config=inference_config, deployment_config=deployment_config, deployment_target=inference_node, overwrite=True)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Start to call an inference\n",
    "Users can call an inference to the endpoint using the operator - service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#Input data - MRPC\n",
    "sentence1 = \"Shares of Genentech, a much larger company with several products on the market, rose more than 2 percent.\"\n",
    "sentence2 = \"Shares of Xoma fell 16 percent in early trade, while shares of Genentech, a much larger company with several products on the market, were up 2 percent.\"\n",
    "input_data = json.dumps({'sentence1':sentence1, 'sentence2': sentence2})\n",
    "\n",
    "try:\n",
    "    aks_return = service.run(input_data)\n",
    "    print(aks_return)\n",
    "    result = aks_return['result']\n",
    "    print('Classification result: ' + str(result))\n",
    "except KeyError as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Clean up\n",
    "Users may wish to delete the deployed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
