{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2022 Intel Corporation\n",
    " \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    " \n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "Unless required by applicable law or agreed to in writing,\n",
    "software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions\n",
    "and limitations under the License.\n",
    " \n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Description\n",
    "\n",
    "Version: 1.0\n",
    "Date: Sep 19, 2022\n",
    "\n",
    "This notebook outlines the general usage of NLP Training using Intel's CPU, PyTorch - with IPEX optimization, and HuggingFace model on Azure Machine Learning platform. A BERT base model is fine-tuned using HuggingFace's trainer class with distributed training and IPEX optimization.\n",
    "\n",
    "Users may wish to base on parts of the code and customize those to suit their purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "Log in Azure - please go to the terminal/console and use the command below to login Azure. Follow the instructions shown in the terminal to perform interactive authentication.\n",
    "\n",
    "Command:\n",
    "'az login'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create an Azure workspace environment to perform the training.\n",
    "\n",
    "Note that some companies may have set some policies in Azure to control the access of the workspaces, storage accounts etc. This may result a deny when creating the workspace using the code provided. If that is the case, users may wish to create the workspace through Azure Machine Learning website manually. \n",
    "\n",
    "After creating the workspace , users may download the config.json through the Microsoft Azure ML website by clicking 'the_workspace_name' -> 'Overview' -> 'Download config.json'\n",
    "\n",
    "After that, copy the config.json to the local environment and re-run the code block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1660854911085
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config('./config.json')\n",
    "    print('Loaded existing workspace configuration')\n",
    "except:\n",
    "    ws = Workspace.create(name='intel_azureml_ws',\n",
    "            subscription_id='----USER AZURE SUBSCRIPTION ID----',  #Please fill in the azure-subscription-id \n",
    "            resource_group='intel_azureml_resource',    #\n",
    "            create_resource_group=True,\n",
    "            location='westus2'\n",
    "            )\n",
    "    ws.write_config(path=\"./\", file_name=\"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create an environment by building a docker image\n",
    "\n",
    "The following code will build a docker image for the cluster to load as the runtime environment. The dockerfile contains all the necessary packages for HugingFace model training and with Intel's optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core import Image\n",
    "azure_ddp_ipex_hf_environment = Environment(name=\"azure_ddp_ipex_hf_environment\")\n",
    "\n",
    "# Specify docker steps as a string. \n",
    "dockerfile = r\"\"\"\n",
    "FROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\n",
    "\n",
    "#Install necessary packages\n",
    "RUN apt-get update && DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get -y install tzdata\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install wget -y && \\\n",
    "    apt-get install python3-pip -y && \\\n",
    "    apt-get install libgl1 -y &&\\\n",
    "    apt-get install python3-opencv -y &&\\\n",
    "    apt-get install git -y &&\\\n",
    "    apt-get install build-essential -y &&\\\n",
    "    apt-get install libtool -y &&\\\n",
    "    apt-get install autoconf -y &&\\\n",
    "    apt-get install unzip -y &&\\\n",
    "    apt-get install libssl-dev -y\n",
    "\n",
    "#Install the PyTorch\n",
    "RUN pip install torch==1.12.1\n",
    "RUN pip install cerberus==1.3.4\n",
    "RUN pip install flatbuffers==2.0\n",
    "RUN pip install h5py==3.7.0\n",
    "RUN pip install numpy==1.23.1\n",
    "RUN pip install packaging==21.3\n",
    "RUN pip install sympy==1.10.1\n",
    "RUN pip install setuptools==63.2.0\n",
    "\n",
    "#Set the environment variable to define protobuf behavior\n",
    "ENV PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION='python'\n",
    "\n",
    "#Install the environment\n",
    "RUN pip install intel-extension-for-pytorch==1.12.100\n",
    "RUN pip install transformers==4.21.1 \n",
    "RUN pip install datasets==2.4.0\n",
    "RUN pip install pandas==1.2.5\n",
    "RUN pip install PyYAML==5.4.1\n",
    "RUN pip install neural-compressor==1.14\n",
    "RUN pip install onnxruntime==1.12.1\n",
    "RUN pip install onnx==1.12.0\n",
    "RUN pip install azureml-defaults #Install the latest package for Azure ML inference/training\n",
    "RUN pip install protobuf==3.20.1 #Need to be in the last to maintain the protobuf version\n",
    "\"\"\"\n",
    "\n",
    "# Set base image to None, because the image is defined by dockerfile.\n",
    "azure_ddp_ipex_hf_environment.docker.base_image = None\n",
    "azure_ddp_ipex_hf_environment.docker.base_dockerfile = dockerfile\n",
    "azure_ddp_ipex_hf_environment.python.user_managed_dependencies=True\n",
    "azure_ddp_ipex_hf_environment.register(workspace=ws) #\n",
    "\n",
    "#Build the docker image\n",
    "build = azure_ddp_ipex_hf_environment.build(workspace=ws)\n",
    "build.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Retrieve the built docker image and set it as the runtime environment of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Environment\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core import Image\n",
    "\n",
    "azure_ddp_ipex_hf_environment = Environment.get(ws, 'azure_ddp_ipex_hf_environment')\n",
    "azure_ddp_ipex_hf_environment.python.user_managed_dependencies=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create a cluster for the distributed training\n",
    "The following code block will create a cluster for the distributed triannig. Users are encouraged to change the variables - 'node_type' and 'num_of_nodes' to manage the cluster type and size. It is recommended to use Intel's IceLake CPU or higher generations to ultilize oneDNN and VNNI instructions.\n",
    "\n",
    "More information regarding to the node_type can be found here:\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1660854914697
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Please change the following 3 variables to suit the use case\n",
    "cpu_cluster_name = \"cpuCluster2xD64DSV4\"\n",
    "node_type = 'STANDARD_D64DS_V4'\n",
    "num_of_nodes = 2\n",
    "\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "except ComputeTargetException:\n",
    "    print('No existing cluster name - ' + cpu_cluster_name + ' . Will start to create a cluster.' )\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=node_type, max_nodes=num_of_nodes) #Ddsv4-series run on the 3rd Generation Intel速 Xeon速 Platinum 8370C (Ice Lake) or the Intel速 Xeon速 Platinum 8272CL (Cascade Lake).\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Start the distributed training\n",
    "The following code block will initiate the AzureML to load the created cluster and environment to start the training job. It is recommended to use the Gloo backend for the PyTorch DDP training. \n",
    "\n",
    "Users are also encouraged to change the content of the train.py to alter the training behavior/suit users' use case. \n",
    "\n",
    "Once the training is completed, the trained model should be downloaded automatically under './fp32_model_output'. If this is not the case, users can locate the PyTorch model file (in HuggingFace format) in the webpage of Azure Machine Learning. 'work_space_name' -> 'Jobs' -> 'the_jobs_id' -> 'Outputs + logs' -> 'outputs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1660858092554
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, ScriptRunConfig, Environment, Experiment\n",
    "from azureml.core.runconfig import MpiConfiguration, PyTorchConfiguration\n",
    "from azureml.core import Workspace\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.core import Dataset\n",
    "\n",
    "distr_config = PyTorchConfiguration(communication_backend='Gloo', process_count=num_of_nodes, node_count=num_of_nodes)\n",
    "compute_target = cpu_cluster\n",
    "\n",
    "script_params = [\n",
    "    '--epochs',\n",
    "    '3',\n",
    "    '--model_name',\n",
    "    'bert-base-uncased',\n",
    "    '--sm-model-dir',\n",
    "    '~/output'\n",
    "]\n",
    "\n",
    "run_config = ScriptRunConfig(\n",
    "  source_directory= '../src/training_container/',\n",
    "  script='train.py',\n",
    "  compute_target=compute_target,\n",
    "  environment=azure_ddp_ipex_hf_environment,\n",
    "  distributed_job_config=distr_config,\n",
    "  arguments = script_params\n",
    ")\n",
    "\n",
    "# submit the run configuration to start the job\n",
    "run = Experiment(ws, \"IntelIPEX_HuggingFace_DDP\").submit(run_config)\n",
    "run.wait_for_completion(show_output=True)\n",
    "run.get_file_names()\n",
    "run.download_files(output_directory='./fp32_model_output', output_paths='outputs2')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
